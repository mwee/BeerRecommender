{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from local_config import REVIEWS_FILE_PATH, BEERS_FILE_PATH\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from multiprocessing import Process, Queue\n",
      "import numpy as np\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import re\n",
      "import random\n",
      "import scipy as sp\n",
      "from scipy import stats\n",
      "from scipy.stats.stats import pearsonr\n",
      "import time\n",
      "\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib as mpl\n",
      "\n",
      "# colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 400\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'white'\n",
      "rcParams['patch.facecolor'] = dark2_colors[0]\n",
      "# rcParams['font.family'] = 'StixGeneral'\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chart junk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    # turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    # now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()\n",
      "        \n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)\n",
      "\n",
      "def autolabel(rects, height_offset, fontsize):\n",
      "    \"\"\"Label rects with their height\"\"\"\n",
      "    for rect in rects:\n",
      "        height = rect.get_height()\n",
      "        plt.text(rect.get_x() + rect.get_width() / 2.0,\n",
      "                 height + height_offset,\n",
      "                 '%d' % int(height),\n",
      "                 ha='center',\n",
      "                 va='bottom',\n",
      "                 rotation='vertical',\n",
      "                 fontsize=fontsize)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################\n",
      "# Define constants #\n",
      "####################\n",
      "\n",
      "DEFAULT_K = 7\n",
      "DEFAULT_REG = 3.0\n",
      "DEFAULT_ASPECT_SCALING_PARAM = 10\n",
      "\n",
      "ASPECTS = ['look', 'smell', 'taste', 'feel', 'overall']\n",
      "RATINGS = [1.0 + 0.25 * x for x in range(17)] # 1-5, in steps of 0.25\n",
      "\n",
      "WORD_SPLIT_REGEX = re.compile(r\"[\\w']+\")\n",
      "SENTENCE_TOKENIZER = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "EXCLUDED_WORDS = set()\n",
      "with open('excluded_words.txt', 'r') as f:\n",
      "    for line in f:\n",
      "        EXCLUDED_WORDS.add(line.strip().lower())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "# Data filtering functions #\n",
      "############################\n",
      "\n",
      "def get_user_averages(df, rating_col_name):\n",
      "    return dict(df.groupby('username')[rating_col_name].mean())\n",
      "\n",
      "def get_single_user_average(df, username, rating_col_name):\n",
      "    return df[df.username == username][rating_col_name].mean()\n",
      "\n",
      "def get_beer_averages(df, rating_col_name):\n",
      "    return dict(df.groupby('beer_id')[rating_col_name].mean())\n",
      "\n",
      "def get_single_beer_average(df, beer_id, rating_col_name):\n",
      "    return df[df.beer_id == beer_id][rating_col_name].mean()\n",
      "\n",
      "def get_thing_rated(thing_id, df):\n",
      "    if isinstance(thing_id, int):\n",
      "        return set(df[df['beer_id'] == beer_id]['username'])\n",
      "    else:\n",
      "        return set(df[df['username'] == username]['beer_id'])\n",
      "    \n",
      "def get_user_top_rated(username, rating_col_name, df, numchoices=5):\n",
      "    \"Return the sorted top numchoices beers for a user by the given rating column name.\"\n",
      "    return df[df['username'] == username][['beer_id', rating_col_name]].sort([rating_col_name], ascending=False).head(numchoices)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################\n",
      "# Data lookup/conversion functions #\n",
      "####################################\n",
      "\n",
      "\"\"\"\n",
      "Beers\n",
      "\"\"\"\n",
      "def beer_id_to_name(beer_id, beer_df):\n",
      "    return beer_df[beer_df['beer_id'] == beer_id].iloc[0]['beer_name']\n",
      "\n",
      "def beer_id_to_brewery_id(beer_id, beer_df):\n",
      "    return beer_df[beer_df['beer_id'] == beer_id].iloc[0]['brewery_id']\n",
      "\n",
      "def beer_name_to_id(beer_name, beer_df):\n",
      "    return beer_df[beer_df['beer_name'] == beer_name].iloc[0]['beer_id']\n",
      "\n",
      "\"\"\"\n",
      "Breweries\n",
      "\"\"\"\n",
      "def brewery_id_to_name(brewery_id, beer_df):\n",
      "    return beer_df[beer_df['brewery_id'] == brewery_id].iloc[0]['brewery_name']\n",
      "\n",
      "def brewery_name_to_id(brewery_name, beer_df):\n",
      "    return beer_df[beer_df['brewery_name'] == brewery_name].iloc[0]['brewery_id']\n",
      "\n",
      "\"\"\"\n",
      "Cross-category\n",
      "\"\"\"\n",
      "def beer_id_to_brewery_name(beer_id, beer_df):\n",
      "    brewery_id = beer_id_to_brewery_id(beer_id, beer_df)\n",
      "    return brewery_id_to_name(brewery_id , beer_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "# Define functions used for common support calculations #\n",
      "#########################################################\n",
      "\n",
      "def get_common_reviewers(beer_id_1, beer_id_2, df):\n",
      "    beer_1_reviewers = df[df.beer_id == beer_id_1].username.unique()\n",
      "    beer_2_reviewers = df[df.beer_id == beer_id_2].username.unique()\n",
      "    return set(beer_1_reviewers).intersection(beer_2_reviewers)\n",
      "\n",
      "def get_common_support(df):\n",
      "    beers = df.beer_id.unique()\n",
      "    print len(beers)\n",
      "    supports = []\n",
      "    for i, beer_id_1 in enumerate(beers):\n",
      "        for j, beer_id_2 in enumerate(beers):\n",
      "            if  i < j:\n",
      "                common_reviewers = get_common_reviewers(beer_id_1, beer_id_2, df)\n",
      "                supports.append(len(common_reviewers))\n",
      "    return supports\n",
      "\n",
      "def get_reviews_for_beer_and_users(beer_id, user_set, df):\n",
      "    \"\"\"\n",
      "    Given a beer id and a set of reviewers, return the sub-dataframe of their reviews.\n",
      "    \"\"\"\n",
      "    mask = (df.username.isin(user_set)) & (df.beer_id == beer_id)\n",
      "    reviews = df[mask]\n",
      "    return reviews[reviews.username.duplicated() == False]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################\n",
      "# Similarity calculation functions #\n",
      "####################################\n",
      "\n",
      "def pearson_sim(beer_1_reviews, beer_2_reviews, user_averages, num_common, rating_col_name):\n",
      "    \"\"\"\n",
      "    Given a subframe of beer 1 reviews and a subframe of beer 2 reviews,\n",
      "    where the reviewers are those who have reviewed both beers, return \n",
      "    the pearson correlation coefficient between the user average subtracted ratings.\n",
      "    The case for zero common reviewers is handled separately. It's\n",
      "    ok to return a NaN if any of the individual variances are 0.\n",
      "    \"\"\"\n",
      "    if num_common == 0:\n",
      "        return 0.0\n",
      "    \n",
      "    diff1 = beer_1_reviews.apply(lambda x: x[rating_col_name] - user_averages[x['username']], axis=1)\n",
      "    diff2 = beer_2_reviews.apply(lambda x: x[rating_col_name] - user_averages[x['username']], axis=1)\n",
      "    \n",
      "    # diff1 = beer_1_reviews[rating_col_name] - beer_1_reviews['user_avg']\n",
      "    # diff2 = beer_2_reviews[rating_col_name] - beer_2_reviews['user_avg']\n",
      "    return pearsonr(diff1, diff2)[0]\n",
      "\n",
      "def calculate_similarity(beer_id_1, beer_id_2, user_averages, similarity_func, rating_col_name, df):\n",
      "    common_reviewers = get_common_reviewers(beer_id_1, beer_id_2, df)\n",
      "    beer_1_reviews = get_reviews_for_beer_and_users(beer_id_1, common_reviewers, df)\n",
      "    beer_2_reviews = get_reviews_for_beer_and_users(beer_id_2, common_reviewers, df)\n",
      "    sim = similarity_func(beer_1_reviews, beer_2_reviews, user_averages, len(common_reviewers), rating_col_name)\n",
      "    return (0.0 if np.isnan(sim) else sim, len(common_reviewers))\n",
      "\n",
      "def shrunk_sim(sim, n_common, reg=DEFAULT_REG):\n",
      "    \"Shrink a similarity by using the regularizer.\"\n",
      "    return (n_common * sim) / (n_common + reg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################################################\n",
      "# Functions for k-nearest neighbors calculations #\n",
      "##################################################\n",
      "\n",
      "\"\"\"\n",
      "Function\n",
      "--------\n",
      "knearest\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "beer_id : string\n",
      "    The id of the beer whose nearest neighbors we want\n",
      "beer_set : array\n",
      "    The set of beers from which we want to find the nearest neighbors\n",
      "db : instance of Database class.\n",
      "    A database of similarities, on which the get method can be used to get the similarity\n",
      "    of two beers. e.g. db.get(rid1,rid2)\n",
      "k : int\n",
      "    the number of nearest neighbors desired\n",
      "reg: float\n",
      "    the regularization.\n",
      "\n",
      "Returns\n",
      "--------\n",
      "A sorted list\n",
      "    of the top k similar beers. The list is a list of tuples\n",
      "    (beer_id, shrunken similarity, common support).\n",
      "\"\"\"\n",
      "def k_nearest_things(thing_id, thing_set, db, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    similar = []\n",
      "    for current_thing_id in thing_set:\n",
      "        if current_thing_id != thing_id:\n",
      "            sim, support = db.get(thing_id, current_thing_id)\n",
      "            similar.append((current_thing_id, shrunk_sim(sim, support, reg=reg), support))\n",
      "    similar.sort(key=lambda x: x[1], reverse=True)\n",
      "    return similar[:k]\n",
      "\n",
      "def k_nearest_thing1s_amongst_thing2_rated(thing1_id, thing2_id, df, db, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    return k_nearest_things(thing1_id, get_user_rated(username, df), db, k=k, reg=reg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "# Recommendation functions #\n",
      "############################\n",
      "\n",
      "def get_top_recos_for_user(username, rating_col_name, df, db, n, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    # we'll get similar beers from all those in the dataset\n",
      "    unique_beer_ids = df['beer_id'].unique()\n",
      "\n",
      "    neighbors = set()\n",
      "    \n",
      "    # for each of the user's top-rated beers...\n",
      "    for i, top_beer_id in get_user_top_rated(username, rating_col_name, df, numchoices=n)['beer_id'].iteritems():\n",
      "        # ...get similar beers\n",
      "        for near_beer_id, _, _ in k_nearest_beers(top_beer_id, unique_beer_ids, db, k=k, reg=reg):\n",
      "            neighbors.add(near_beer_id)\n",
      "\n",
      "    # only use beers that the user has not reviewed\n",
      "    neighbors = neighbors - get_user_rated(username, df)\n",
      "    \n",
      "    result = [(beer_id, get_single_beer_average(df, beer_id, rating_col_name)) for beer_id in neighbors]\n",
      "    return sorted(result, key=lambda x: x[1], reverse=True)[:n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#####################\n",
      "# Rating prediction #\n",
      "#####################\n",
      "\n",
      "def baseline(global_avg, user_avg, beer_avg):\n",
      "    return global_avg + (user_avg - global_avg) + (beer_avg - global_avg)\n",
      "\n",
      "def predict_rating(beer_id, username, rating_col_name, db, df, k=DEFAULT_K, reg=DEFAULT_REG):\n",
      "    global_avg = df[rating_col_name].mean()\n",
      "    user_avg = get_single_user_average(df, username, rating_col_name)\n",
      "    beer_avg = get_single_beer_average(df, beer_id, rating_col_name)\n",
      "    \n",
      "    nearest = k_nearest_beers_amongst_user_rated(beer_id, username, df, db, k, reg)\n",
      "    num = 0.0\n",
      "    denom = 0.0\n",
      "    for near_beer_id, sim, support in nearest:\n",
      "        reviews = df[(df['username'] == username) & (df['beer_id'] == near_beer_id)]\n",
      "        assert(reviews.shape[0] == 1)\n",
      "        near_beer_avg = get_single_beer_average(df, near_beer_id, rating_col_name)\n",
      "        num += sim * (float(reviews.irow(0)[rating_col_name]) - baseline(global_avg, user_avg, near_beer_avg))\n",
      "        denom += sim\n",
      "    \n",
      "    return baseline(global_avg, user_avg, beer_avg) + num / denom\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################\n",
      "# Aspect weighting #\n",
      "####################\n",
      "\n",
      "def normalize(a):\n",
      "    s = float(sum(a))\n",
      "    return [0.0 if x == 0.0 else float(x) / s for x in a]\n",
      "\n",
      "def get_aspect_weights(username, df, scale=DEFAULT_ASPECT_SCALING_PARAM):    \n",
      "    user_reviews = df[df['username'] == username]\n",
      "    overall_ratings = user_reviews['overall']\n",
      "    \n",
      "    scaling_const = min(1.0, float(len(user_reviews)) / float(scale))\n",
      "    \n",
      "    aspects = ['look', 'smell', 'taste', 'feel']\n",
      "    weights = [max(0.0, pearsonr(user_reviews[aspect], overall_ratings)[0] * scaling_const) for aspect in aspects]\n",
      "    \n",
      "    return tuple(normalize(weights))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Database:\n",
      "    \"\"\"A class representing a database of similaries and common supports.\"\"\"\n",
      "    def __init__(self, df):\n",
      "        self.df = df\n",
      "        \n",
      "        self.unique_beer_ids = {v: k for (k, v) in enumerate(df.beer_id.unique())}\n",
      "        beer_keys = self.unique_beer_ids.keys()\n",
      "        num_beer_keys = len(beer_keys)\n",
      "        \n",
      "        self.database_beer_sim = np.zeros([num_beer_keys, num_beer_keys])\n",
      "        self.database_beer_sup = np.zeros([num_beer_keys, num_beer_keys], dtype=np.int)\n",
      "        \n",
      "        self.unique_usernames = {v: k for (k, v) in enumerate(df.username.unique())}\n",
      "        user_keys = self.unique_usernames.keys()\n",
      "        num_user_keys = len(user_keys)\n",
      "        \n",
      "        self.database_user_sim = np.zeros([num_user_keys, num_user_keys])\n",
      "        self.database_user_sup = np.zeros([num_user_keys, num_user_keys], dtype=np.int)\n",
      "    \n",
      "    def populate_by_calculating(self, similarity_func, rating_col_name):\n",
      "        \"\"\"\n",
      "        a populator for every pair of businesses and every pair of users\n",
      "        in df. takes similarity_func like pearson_sim as argument\n",
      "        \"\"\"\n",
      "        \n",
      "        # BEERS\n",
      "        user_averages = get_user_averages(self.df, rating_col_name)\n",
      "        \n",
      "        items = self.unique_beer_ids.items()\n",
      "        \n",
      "        count = 0\n",
      "        for beer_id_1, i in items:\n",
      "            print count, i\n",
      "            count += 1\n",
      "            for beer_id_2, j in items:\n",
      "                if i < j:\n",
      "                    sim, nsup = calculate_similarity(beer_id_1, beer_id_2, user_averages, similarity_func, rating_col_name, self.df)\n",
      "                    self.database_beer_sim[i][j] = sim\n",
      "                    self.database_beer_sim[j][i] = sim\n",
      "                    self.database_beer_sup[i][j] = nsup\n",
      "                    self.database_beer_sup[j][i] = nsup\n",
      "                elif i == j:\n",
      "                    nsup = self.df[self.df.beer_id == beer_id_1].username.count()\n",
      "                    self.database_sim[i][i] = 1.0\n",
      "                    self.database_sup[i][i] = nsup\n",
      "                    \n",
      "        # USERS\n",
      "        beer_averages = get_beer_averages(self.df, rating_col_name)\n",
      "        \n",
      "        items = self.unique_usernames.items()\n",
      "        \n",
      "        ##############\n",
      "        ##############\n",
      "        ##############\n",
      "        # I DONT KNOW WHAT TO DO HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "        # I DONT KNOW WHAT TO DO HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "        # I DONT KNOW WHAT TO DO HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "        # I DONT KNOW WHAT TO DO HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "        # I DONT KNOW WHAT TO DO HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "        with open('populate_database_mr_input.txt', 'w') as f:\n",
      "            for beer_id_1, i in items:\n",
      "                for beer_id_2, j in items:\n",
      "                    f.write('%s %s %s %s\\n' % (beer_id_1, beer_id_2, i, j))\n",
      "        \n",
      "        print 'DONE'\n",
      "        \n",
      "        count = 0\n",
      "        for username_1, i in items:\n",
      "            print count, i\n",
      "            count += 1\n",
      "            for username_2, j in items:\n",
      "                if i < j:\n",
      "                    sim, nsup = calculate_similarity(username_1, username_2, beer_averages, similarity_func, rating_col_name, self.df)\n",
      "                    self.database_user_sim[i][j] = sim\n",
      "                    self.database_user_sim[j][i] = sim\n",
      "                    self.database_user_sup[i][j] = nsup\n",
      "                    self.database_user_sup[j][i] = nsup\n",
      "                elif i == j:\n",
      "                    nsup = self.df[self.df.username == username_1].beer_id.count()\n",
      "                    self.database_user_sim[i][i] = 1.0\n",
      "                    self.database_user_sup[i][i] = nsup\n",
      "                    \n",
      "    def get(self, beer_id_1, beer_id_2):\n",
      "        \"returns a tuple of similarity,common_support given two business ids\"\n",
      "        sim = self.database_sim[self.unique_beer_ids[beer_id_1]][self.unique_beer_ids[beer_id_2]]\n",
      "        nsup = self.database_sup[self.unique_beer_ids[beer_id_1]][self.unique_beer_ids[beer_id_2]]\n",
      "        return (sim, nsup)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################################\n",
      "# Read in reviews and beers data #\n",
      "##################################\n",
      "\n",
      "reviews_df_raw = pd.read_csv(REVIEWS_FILE_PATH)\n",
      "beer_df = pd.read_csv(BEERS_FILE_PATH)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print reviews_df_raw\n",
      "reviews_df = reviews_df_raw[pd.notnull(reviews_df_raw['username'])]\n",
      "reviews_df = reviews_df[pd.notnull(reviews_df['text'])]\n",
      "\n",
      "# filter out users with a small number of reviews\n",
      "filtered_usernames = []\n",
      "username_groups = reviews_df.groupby('username')\n",
      "for username, reviews in username_groups:\n",
      "    if len(reviews) > 80:\n",
      "        filtered_usernames.append(username)\n",
      "reviews_df = reviews_df[reviews_df['username'].isin(filtered_usernames)]\n",
      "\n",
      "# filter out beers with a small number of reviews\n",
      "filtered_beer_ids = []\n",
      "beer_groups = reviews_df.groupby('beer_id')\n",
      "for beer_id, reviews in beer_groups:\n",
      "    if len(reviews) > 450:\n",
      "        filtered_beer_ids.append(beer_id)\n",
      "reviews_df = reviews_df[reviews_df['beer_id'].isin(filtered_beer_ids)]\n",
      "\n",
      "print ''\n",
      "print 'Num users:', len(reviews_df['username'].unique())\n",
      "print 'Num beers:', len(reviews_df['beer_id'].unique())\n",
      "\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]['brewery_id']\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]['beer_id']\n",
      "# print reviews_df[pd.isnull(reviews_df['username'])]['text']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db = Database(reviews_df)\n",
      "db.populate_by_calculating(pearson_sim, 'rating')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reviews_df.to_csv('reviews_subset.csv', index=False)\n",
      "\n",
      "# get a copy of the raw df, but with null usernames and review text filtered out\n",
      "full_reviews_df = reviews_df_raw[pd.notnull(reviews_df_raw['username'])]\n",
      "full_reviews_df = full_reviews_df[pd.notnull(full_reviews_df['text'])]\n",
      "\n",
      "def convert_to_mr_format(df):\n",
      "    # get DataFrame-wide user averages\n",
      "    user_averages = {}\n",
      "    for aspect in ASPECTS:\n",
      "        user_averages[aspect] = get_user_averages(df, aspect)\n",
      "        \n",
      "    with open('mr_input/mr_input_all_full.txt', 'w') as f:\n",
      "        for i, review in df.iterrows():\n",
      "            username = review['username']\n",
      "            beer_id = review['beer_id']\n",
      "            \n",
      "            things = [username, beer_id]\n",
      "            for aspect in ASPECTS:\n",
      "                things.append(review[aspect] - user_averages[aspect][username])\n",
      "            \n",
      "            f.write(' '.join([str(x) for x in things]) + '\\n')\n",
      "\n",
      "convert_to_mr_format(full_reviews_df)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print out the names and ids of all the beers in the dataset\n",
      "# beers_data = []\n",
      "# for beer_id in reviews_df['beer_id'].unique():\n",
      "#     beers_data.append((beer_id, beer_id_to_name(beer_id, beer_df), beer_id_to_brewery_name(beer_id, beer_df)))\n",
      "# for beer_id, beer_name, brewery_name in sorted(beers_data, key=lambda x: x[2]):    \n",
      "#     print str(beer_id).ljust(6, ' '), brewery_name, '|', beer_name\n",
      "\n",
      "# print out all usernames in the dataset, sorted by number of reviews\n",
      "username_groups = reviews_df.groupby('username')\n",
      "user_data = []\n",
      "for username, reviews in username_groups:\n",
      "    user_data.append((username, len(reviews)))\n",
      "for username, num_reviews in sorted(user_data, key=lambda x: x[1], reverse=True):\n",
      "    # look, smell, taste, feel = get_aspect_weights(username, reviews_df)\n",
      "    # look < smell, look < taste, look < feel\n",
      "    \n",
      "    # print num_reviews, username.ljust(10), get_aspect_weights(username, reviews_df)\n",
      "    pass\n",
      "\n",
      "for scale in range(5, 6):\n",
      "    correlations = []\n",
      "    for username, reviews in username_groups:\n",
      "        look, smell, taste, feel = get_aspect_weights(username, reviews_df, scale=scale)\n",
      "        \n",
      "        predicted = np.array(reviews['look']) * look \\\n",
      "            + np.array(reviews['smell']) * smell \\\n",
      "            + np.array(reviews['taste']) * taste \\\n",
      "            + np.array(reviews['feel']) * feel\n",
      "        actual = np.array(reviews['overall'])\n",
      "        \n",
      "        corr = pearsonr(predicted, actual)[0]\n",
      "        if pd.notnull(corr):\n",
      "            correlations.append(corr)\n",
      "    \n",
      "    plt.hist(correlations, bins=40)\n",
      "    print sum(correlations), scale\n",
      "    \n",
      "    # print sorted(correlations, reverse=False)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_beer_id = 58577\n",
      "nearest_beers = k_nearest_things(test_beer_id, reviews_df['beer_id'].unique(), db)\n",
      "\n",
      "print 'Top matches for %s (%s):' % (beer_id_to_name(test_beer_id, beer_df), test_beer_id)\n",
      "for i, (beer_id, sim, support) in enumerate(nearest_beers):\n",
      "    print i, beer_id_to_name(beer_id, beer_df), \"| Sim\", sim, \"| Support\", support"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_username = 'Sammy'\n",
      "nearest_users = k_nearest_things(test_username, reviews_df['username'].unique(), db)\n",
      "\n",
      "print 'Top matches for %s (%s):' % username\n",
      "for i, (username, sim, support) in enumerate(nearest_users):\n",
      "    print i, username, \"| Sim\", sim, \"| Support\", support\n",
      "\n",
      "#for beer_id, avg in get_top_recos_for_user(test_username, 'rating', reviews_df, db, n=10):\n",
      " #   print str(round(avg, 3)).ljust(5), beer_id_to_name(beer_id, beer_df), '(' + beer_id_to_brewery_name(beer_id, beer_df) + ')'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_df = reviews_df.iloc[0:7000]\n",
      "\n",
      "class SentenceModel(object):\n",
      "    def __init__(self, df):\n",
      "        self.df = df\n",
      "        self.sentences = {}\n",
      "        self.words_set = set()\n",
      "        self.ratings = {}\n",
      "        \n",
      "        # some useful numbers\n",
      "        self.num_reviews = 0\n",
      "        self.num_rating_possibilites = len(ASPECTS) * len(RATINGS)\n",
      "        \n",
      "        # initialize sentences and words\n",
      "        for i, review in df.iterrows():\n",
      "            self.num_reviews += 1\n",
      "            \n",
      "            for s in SENTENCE_TOKENIZER.tokenize(review['text']):\n",
      "                # Don't use sentences that just describe the serving type\n",
      "                if s.startswith('Serving type: '):\n",
      "                    break\n",
      "                    \n",
      "                # get all words in the sentence\n",
      "                words = set([w.lower() for w in WORD_SPLIT_REGEX.findall(s)])\n",
      "                \n",
      "                # remove common \"function\" words\n",
      "                words -= EXCLUDED_WORDS\n",
      "                \n",
      "                # add the sentence to the sentence dict\n",
      "                if i not in self.sentences:\n",
      "                    self.sentences[i] = []\n",
      "                self.sentences[i].append([s, words, None])\n",
      "                \n",
      "                # add the words to the global word set\n",
      "                self.words_set.update(words)\n",
      "\n",
      "            # record this review's ratings\n",
      "            self.ratings[i] = {}\n",
      "            for k in ASPECTS:\n",
      "                self.ratings[i][k] = review[k]\n",
      "        \n",
      "        # initialize aspect weights\n",
      "        self.theta = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.theta[aspect] = {}\n",
      "            for w in self.words_set:\n",
      "                self.theta[aspect][w] = random.random() / 2.0\n",
      "        for aspect in ASPECTS:\n",
      "            self.theta[aspect][aspect] = 1.0\n",
      "        \n",
      "        # initialize sentiment weights\n",
      "        self.phi = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.phi[aspect] = {}\n",
      "            for r in RATINGS:\n",
      "                self.phi[aspect][r] = {}\n",
      "                for w in self.words_set:\n",
      "                   self.phi[aspect][r][w] = random.random() / 2.0\n",
      "        for aspect in ASPECTS:\n",
      "            self.phi[aspect][3.0][aspect] = 1\n",
      "        \n",
      "        # will be used later for gradient ascent\n",
      "        self.gradient_theta = {}\n",
      "        self.gradient_phi = {}\n",
      "        \n",
      "    def __iter__(self):\n",
      "        for df_index, sentences in self.sentences.iteritems():\n",
      "            for sentence_num, sentence_data in enumerate(sentences):\n",
      "                yield (df_index, sentence_num, sentence_data[0], sentence_data[1], sentence_data[2])\n",
      "\n",
      "    def get_sentence_prob(self, i, j, aspect):\n",
      "        words = self.get_words(i, j)\n",
      "        \n",
      "        z = 0.0\n",
      "        this_aspect_sum = None\n",
      "        for k in ASPECTS:\n",
      "            weight_sum = 0.0\n",
      "            for w in words:\n",
      "                weight_sum += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "            \n",
      "            if k == aspect:\n",
      "                this_aspect_sum = weight_sum\n",
      "                \n",
      "            z += np.exp(weight_sum)\n",
      "                \n",
      "        return np.exp(this_aspect_sum) / z\n",
      "                \n",
      "#     def _update_assignments(self):\n",
      "#         NUM_WORKERS = 4\n",
      "        \n",
      "#         def worker(request_queue, result_queue, worker_num):\n",
      "#             changed = False\n",
      "#             for data in iter(request_queue.get, None):\n",
      "#                 i, j = data\n",
      "                \n",
      "#                 max_val = float('-inf')\n",
      "#                 max_aspect = None\n",
      "#                 for k in ASPECTS:\n",
      "#                     p = np.log(self.get_sentence_prob(i, j, k))\n",
      "#                     if p > max_val:\n",
      "#                         max_val = p\n",
      "#                         max_aspect = k\n",
      "                \n",
      "#                 if self.get_aspect(i, j) != max_aspect:\n",
      "#                     changed = True\n",
      "#                     self.set_aspect(i, j, max_aspect)\n",
      "            \n",
      "#             result_queue.put(changed)\n",
      "        \n",
      "#         request_queue = Queue()\n",
      "#         result_queue = Queue()\n",
      "        \n",
      "#         # start workers\n",
      "#         workers = []\n",
      "#         for i in range(NUM_WORKERS):\n",
      "#             w = Process(target=worker, args=(request_queue, result_queue, i,))\n",
      "#             workers.append(w)\n",
      "#             w.start()\n",
      "        \n",
      "#         # put data and terminate flags in the queue\n",
      "#         for i, j, _, _, _ in self:\n",
      "#             request_queue.put((i, j,))\n",
      "#         for i in range(NUM_WORKERS):\n",
      "#             request_queue.put(None)\n",
      "        \n",
      "#         # get results\n",
      "#         results = []\n",
      "#         num_read = 0\n",
      "#         while num_read < NUM_WORKERS:\n",
      "#             result = result_queue.get()\n",
      "#             results.append(result)\n",
      "#             print result\n",
      "#             num_read += 1\n",
      "        \n",
      "#         # wait until all workers finish\n",
      "#         for w in workers:\n",
      "#             w.join()\n",
      "        \n",
      "#         return any(results)\n",
      "\n",
      "    def _update_assignments(self):\n",
      "        changed = False\n",
      "        for i, j, _, _, _ in self:\n",
      "            max_val = float('-inf')\n",
      "            max_aspect = None\n",
      "            for k in ASPECTS:\n",
      "                p = np.log(self.get_sentence_prob(i, j, k))\n",
      "                if p > max_val:\n",
      "                    max_val = p\n",
      "                    max_aspect = k\n",
      "            \n",
      "            if self.get_aspect(i, j) != max_aspect:\n",
      "                changed = True\n",
      "                self.set_aspect(i, j, max_aspect)\n",
      "        \n",
      "        return changed\n",
      "    \n",
      "    def _init_gradient_dicts(self):\n",
      "        self.gradient_theta = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.gradient_theta[aspect] = {}\n",
      "            for w in self.words_set:\n",
      "                self.gradient_theta[aspect][w] = 0.0\n",
      "        \n",
      "        self.gradient_phi = {}\n",
      "        for aspect in ASPECTS:\n",
      "            self.gradient_phi[aspect] = {}\n",
      "            for r in RATINGS:\n",
      "                self.gradient_phi[aspect][r] = {}\n",
      "                for w in self.words_set:\n",
      "                   self.gradient_phi[aspect][r][w] = 0\n",
      "    \n",
      "    def _compute_gradient(self):\n",
      "        for k in ASPECTS:\n",
      "            for w in self.words_set:\n",
      "                self.gradient_theta[k][w] = -1.0 * float(self.num_rating_possibilites) * self.theta[k][w]\n",
      "                \n",
      "                for r in RATINGS:\n",
      "                    self.gradient_phi[k][r][w] = -1.0 * float(self.num_rating_possibilites) * self.phi[k][r][w] \n",
      "    \n",
      "        for i, j, s, words, curr_aspect in self:\n",
      "            curr_aspect_rating = self.ratings[i][curr_aspect]\n",
      "            \n",
      "            num = 0.0\n",
      "            denom = 0.0\n",
      "            for k in ASPECTS:\n",
      "                exp_score = 0.0\n",
      "                for w in words:\n",
      "                    exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                exp_score = np.exp(exp_score)\n",
      "                \n",
      "                if k == curr_aspect:\n",
      "                    num = exp_score\n",
      "                \n",
      "                denom += exp_score\n",
      "            \n",
      "            frac = num / denom\n",
      "            for w in words:\n",
      "                self.gradient_theta[curr_aspect][w] += 1.0 - frac\n",
      "                self.gradient_phi[curr_aspect][curr_aspect_rating][w] += 1.0 - frac\n",
      "    \n",
      "#     def _compute_log_likelihood(self):\n",
      "#         likelihood = 0.0\n",
      "        \n",
      "#         for i, j, s, words, curr_aspect in self:\n",
      "#             denom = 0.0\n",
      "#             for k in ASPECTS:\n",
      "#                 exp_score = 0.0\n",
      "#                 for w in words:\n",
      "#                     exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                \n",
      "#                 if k == curr_aspect:\n",
      "#                     likelihood += exp_score\n",
      "#                 exp_score = np.exp(exp_score)\n",
      "#                 denom += exp_score\n",
      "#             likelihood -= np.log(denom)\n",
      "        \n",
      "#         return likelihood\n",
      "    \n",
      "    def _compute_log_likelihood(self):\n",
      "        NUM_WORKERS = 4\n",
      "        \n",
      "        def worker(request_queue, result_queue, worker_num):\n",
      "            likelihood = 0.0\n",
      "            \n",
      "            for data in iter(request_queue.get, None):\n",
      "                i, j, s, words, curr_aspect = data\n",
      "                \n",
      "                denom = 0.0\n",
      "                for k in ASPECTS:\n",
      "                    exp_score = 0.0\n",
      "                    for w in words:\n",
      "                        exp_score += self.theta[k][w] + self.phi[k][self.ratings[i][k]][w]\n",
      "                    \n",
      "                    if k == curr_aspect:\n",
      "                        likelihood += exp_score\n",
      "                    exp_score = np.exp(exp_score)\n",
      "                    denom += exp_score\n",
      "                likelihood -= np.log(denom)\n",
      "            \n",
      "            result_queue.put(likelihood)\n",
      "        \n",
      "        request_queue = Queue()\n",
      "        result_queue = Queue()\n",
      "        \n",
      "        # start workers\n",
      "        workers = []\n",
      "        for i in range(NUM_WORKERS):\n",
      "            w = Process(target=worker, args=(request_queue, result_queue, i,))\n",
      "            workers.append(w)\n",
      "            w.start()\n",
      "        \n",
      "        # put data and terminate flags in the queue\n",
      "        for i, j, s, words, curr_aspect in self:\n",
      "            request_queue.put((i, j, s, words, curr_aspect))\n",
      "        for i in range(NUM_WORKERS):\n",
      "            request_queue.put(None)\n",
      "        \n",
      "        # get results\n",
      "        likelihood = 0.0\n",
      "        num_read = 0\n",
      "        while num_read < NUM_WORKERS:\n",
      "            result = result_queue.get()\n",
      "            likelihood += result\n",
      "            num_read += 1\n",
      "        \n",
      "        # wait until all workers finish\n",
      "        for w in workers:\n",
      "            w.join()\n",
      "        \n",
      "        return likelihood\n",
      "    \n",
      "    def train(self, learning_rate=None, iterations=10, gradient_ascent_iterations=5):\n",
      "        if not learning_rate:\n",
      "            learning_rate = float(self.num_rating_possibilites) * 0.01 / float(self.num_reviews)\n",
      "            print 'Defaulting to learning rate of %s' % learning_rate\n",
      "        \n",
      "        self._init_gradient_dicts()\n",
      "        \n",
      "        likelihood = 0.0\n",
      "        prev_likelihood = 0.0\n",
      "        \n",
      "        # TODO accuracy, kappa?\n",
      "        # TODO gradient iter num\n",
      "        \n",
      "        for iter_num in range(iterations):\n",
      "            main_iter_start = time.time()\n",
      "            \n",
      "            print 'Main iter %s' % iter_num\n",
      "            print '    Updating assignments...'\n",
      "            changed = self._update_assignments()\n",
      "            \n",
      "            # TODO eval accuracy?\n",
      "            \n",
      "            # if the model didn't change, no need to keep going\n",
      "            # TODO add gradient iter num condition here\n",
      "            if (not changed):\n",
      "                print '    Assignments did not change; breaking'\n",
      "                break\n",
      "            else:\n",
      "                print '    Assignments changed'\n",
      "            \n",
      "            likelihood = self._compute_log_likelihood()\n",
      "            if i == 0:\n",
      "                prev_likelihood = likelihood\n",
      "            print '    Starting likelihood: %s' % likelihood\n",
      "            \n",
      "            for g_iter_num in range(gradient_ascent_iterations):\n",
      "                g_iter_start = time.time()\n",
      "                \n",
      "                print '    Gradient ascent iter %s' % g_iter_num\n",
      "                prev_likelihood = likelihood\n",
      "                self._compute_gradient()\n",
      "                \n",
      "                # do the actual gradient ascent\n",
      "                for k in ASPECTS:\n",
      "                    for w in self.words_set:\n",
      "                        self.theta[k][w] += learning_rate * self.gradient_theta[k][w]\n",
      "                        for r in RATINGS:\n",
      "                            self.phi[k][r][w] += learning_rate * self.gradient_phi[k][r][w]\n",
      "                \n",
      "                likelihood = self._compute_log_likelihood()\n",
      "                print '        Likelihood: %s' % likelihood\n",
      "                \n",
      "                # undo the last operation if likelihood didn't improve\n",
      "                if not (likelihood > prev_likelihood):\n",
      "                    print '        Likelihood did not improve; undoing and breaking'\n",
      "                    for k in ASPECTS:\n",
      "                        for w in self.words_set:\n",
      "                            self.theta[k][w] -= learning_rate * self.gradient_theta[k][w]\n",
      "                            for r in RATINGS:\n",
      "                                self.phi[k][r][w] -= learning_rate * self.gradient_phi[k][r][w]\n",
      "                    likelihood = prev_likelihood\n",
      "                    break\n",
      "                \n",
      "                print '        Time: %s s' % (time.time() - g_iter_start)\n",
      "            \n",
      "            foo = {}\n",
      "            for k in ASPECTS:\n",
      "                foo[k] = {}\n",
      "                for w in self.words_set:\n",
      "                    foo[k][w] = 0.0\n",
      "                    for r in RATINGS:\n",
      "                        foo[k][w] += self.phi[k][r][w]\n",
      "                    foo[k][w] /= len(RATINGS)\n",
      "            \n",
      "            for k in ASPECTS:\n",
      "                for w in self.words_set:\n",
      "                    for r in RATINGS:\n",
      "                        self.phi[k][r][w] -= foo[k][w]\n",
      "                    self.theta[k][w] += foo[k][w]\n",
      "            \n",
      "            prev_likelihood = likelihood\n",
      "            \n",
      "            print '    Likelihood: %s' % likelihood\n",
      "            print '    Time: %s s' % (time.time() - main_iter_start)\n",
      "            # TODO eval accuracy?\n",
      "            # TIOD update best_likelihood\n",
      "    \n",
      "    def get_aspect(self, i, j):\n",
      "        return self.sentences[i][j][2]\n",
      "    \n",
      "    def set_aspect(self, i, j, aspect):\n",
      "        self.sentences[i][j][2] = aspect\n",
      "        \n",
      "    def get_words(self, i, j):\n",
      "        return self.sentences[i][j][1]\n",
      "\n",
      "sentence_model = SentenceModel(reviews_df)\n",
      "# sentence_model = SentenceModel(small_df)\n",
      "print len(sentence_model.words_set)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence_model.train(iterations=20, gradient_ascent_iterations=10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, values in sentence_model.theta.iteritems():\n",
      "    print k.upper() + ':'\n",
      "    for word, weight in sorted(values.iteritems(), key=lambda x: x[1], reverse=True)[:10]:\n",
      "        print '    %s: %s' % (word, weight)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}